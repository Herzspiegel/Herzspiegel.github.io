---
title: '{DR 02} Choose a Transformer: Fourier or Galerkin'
date: 2024-08-04
permalink: /posts/2012/08/blog-post-1/
tags:
  - Transformer
  - Weight Residual Method
  - Attention Layer
---

Connections: Transformer, SVD, Matrix Multiplication

Headings are cool
======

You can have many headings
======

Background Introduction
------
Scientists and engineers have been working on **approximating the governing PDEs** of these physical systems for centuries.
- Traditional Method: leverage a discrete structure to reduce an infinite dimensional operator map to a finite dimensional approximation problem
- Neural Operator: A well-trained operator learner can **evaluate many instances** without re-training or collocation points.
> Mesh-invariant, Generalization in a branch of instances from a distribution.

Related Works
------
- Operator learners related to PDEs: Different introduced inductive bias and corresponding methods (locate the replacement of his method)
- Attention mechanism and variants: Modifying attention mechanism and the history of removing softmax (locate the unique design of his method)
- Various studies on Transformers: different perspective of analysing transformer (locate his unique/novel explaination)

Problem Setup
------
- Summary of the tasks in Operator learning
- Not making any assumption for modeling this map (Next section directly use different Transformer structures and give them different explainations)_
