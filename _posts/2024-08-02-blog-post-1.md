---
title: '{DR 01} Analysis on the Number of Linear Regions of Piecewise Linear Neural Networks'
date: 2024-08-02
permalink: /posts/2024/0802/blog-post-1/
tags:
  - expressive power
  - piecewise linear
---

Connections: FEM, HiDeNN, VC-dim, KAN

Comments: Fuzzy description but insightful

Interesting points (Abstract)
------

1. For piecewise linear neural networks (PLNNs), **the number of linear regions** is a natural measure of their expressive power since it characterizes **the number of linear pieces available to model complex patterns**.
> Note: A "pattern" perspective to measure the expressive power of "simplified" Deep Neural Network

2. This key property **(O(Deeper) > O(Wider))** enables deep PLNNs with complex activation functions to outperform their shallow counterparts (**experiment?**) when computing highly complex and **structured** functions, which, to some extent, explains the performance improvement of deep PLNNs in classification and function fitting.
> How to explain number of "linear regions" => expressive power? more components?
> 
> If not direct number, but compare two trees? exp/log? -- correlation?

PLNN (Prelim / Front matter)
------
1. Piecewise linear activation function: **approximation of non-linear activation function**, continuous (use piece cut to behaviour as mesh-invariant) (analytical form)
> Transolver: learnable activation function WITH Softmax()

Things become interesting ...
