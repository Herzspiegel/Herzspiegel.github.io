---
title: '{DR 02} Choose a Transformer: Fourier or Galerkin'
date: 2024-08-04
permalink: /posts/2012/08/blog-post-1/
tags:
  - Transformer
  - Weight Residual Method
  - Attention Layer
---

This is a sample blog post. Lorem ipsum I can't remember the rest of lorem ipsum and don't have an internet connection right now. Testing testing testing this blog post. Blog posts are cool.

Headings are cool
======

You can have many headings
======

Background Introduction
------
Scientists and engineers have been working on **approximating the governing PDEs** of these physical systems for centuries.
- Traditional Method: leverage a discrete structure to reduce an infinite dimensional operator map to a finite dimensional approximation problem
- Neural Operator: A well-trained operator learner can **evaluate many instances** without re-training or collocation points.
> Mesh-invariant, Generalization in a branch of instances from a distribution.

Related Works
------
- Operator learners related to PDEs: Different introduced inductive bias and corresponding methods (locate the replacement of his method)
- Attention mechanism and variants: Modifying attention mechanism and the history of removing softmax (locate the unique design of his method)
- Various studies on Transformers: different perspective of analysing transformer (locate his unique/novel explaination)

Problem Setup
------
Summary of the tasks in Operator learning
