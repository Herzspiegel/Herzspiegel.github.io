---
title: 'DR 01 | Analysis on the Number of Linear Regions of Piecewise Linear Neural Networks'
date: 2024-08-02
permalink: /posts/2024/0802/blog-post-1/
tags:
  - expressive power
  - piecewise linear
---

Connections: FEM, HiDeNN, VC-dim, KAN

Comments: Fuzzy description but insightful

Interesting points (Abstract)
------

1. For piecewise linear neural networks (PLNNs), **the number of linear regions** is a natural measure of their expressive power since it characterizes **the number of linear pieces available to model complex patterns**.
> Note: A "pattern" perspective to measure the expressive power of "simplified" Deep Neural Network

2. This key property **(O(Deeper) > O(Wider))** enables deep PLNNs with complex activation functions to outperform their shallow counterparts (**experiment?**) when computing highly complex and **structured** functions, which, to some extent, explains the performance improvement of **deep** PLNNs in classification and function fitting.
> Q: How to explain number of "linear regions" => expressive power? more components?
> 
>> If not direct number, but compare two trees? exp/log? -- correlation?

PLNN (Prelim / Front matter)
------
1. Piecewise linear activation function: **approximation of non-linear activation function**, continuous (use piece-cut to be mesh-invariant) (with a specific analytical form)
> Transolver: Slice weights = learnable activation function WITH Softmax() (depend on data [conditional])
>
> If different standard to different neuron to activate?
2. Linear Regions
> From Wiki ([Margin](https://en.wikipedia.org/wiki/Margin_(machine_learning))): If such a hyperplane exists, it is known as the maximum-margin hyperplane and the linear classifier it defines is known as a maximum margin classifier; or equivalently, **the perceptron of optimal stability**.
>
> Kind of "feature engneering"

Things become interesting ...
